## ğŸ“˜ RAG Parameter Exploration Assignment

Pada tugas ini kamu akan bereksperimen dengan **5 parameter utama Retrieval-Augmented Generation (RAG)** + **opsi model Groq**.

Kamu akan:

âœ… memahami efek tuning RAG

âœ… mengamati perubahan kualitas vs kecepatan

âœ… belajar trade-off retrieval & generation

âœ… menulis insight praktis dari eksperimen

> Tools sudah disediakan. Kamu **tidak perlu ngoding**, hanya ubah nilai parameter di file Python & restart server.

**Notes:**

**- Ikuti video di LMS jika kamu kebingungan.**

**- Sediakan penyimpanan at least 2GB, bakalan ada download model yang cukup besar ke laptopmu.**

**- Hapus proyek ini kalau sudah selesai, biar laptop mu tidak tercekik**

**- Ketika proyek ini sudah diisi, sudah dikerjakan dibawah, maka upload ke GitHub.**

---

## âš™ï¸ Parameter yang diuji

### 1ï¸âƒ£ Ukuran chunk

```python
CHUNK_SIZE = 300
```

**Fungsi**
Mengatur seberapa besar potongan teks untuk embedding.

| Arah tuning               | Efek                                                                 |
| ------------------------- | -------------------------------------------------------------------- |
| **Lebih besar** (400â€“600) | jawabannya langsung to the point tentang AR dan alasan pemilihannya. |
| **Lebih kecil** (150â€“250) | jawabannya melebar ke banyak hal lain yang sebenarnya tidak ditanya. |

**Pertanyaan**

- Chunk besar atau kecil lebih tepat untuk dokumen yang kamu tes?
- Jawab : menggunakan chunk besar lebih cocok.

---

### 2ï¸âƒ£ Kandidat awal vector search

```python
TOP_K_INITIAL = 20
```

**Fungsi**
Berapa dokumen paling mirip yang diambil sebelum filtering.

| Arah tuning         | Efek                                                                                                                                                                 |
| ------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Naikkan** (30â€“50) | Sistem mengambil lebih banyak kandidat dokumen sebelum memilih yang terbaik. Hasilnya lebih aman untuk pertanyaan yang agak luas, tapi prosesnya sedikit lebih lama. |
| **Turunkan** (5â€“10) | Sistem hanya mengambil sedikit kandidat dokumen. Lebih cepat, tapi berisiko kehilangan konteks penting jika dokumen yang cocok tidak termasuk dalam 5â€“10 pertama.    |

**Pertanyaan**

- Apakah top-k kecil cukup untuk dataset kamu?
- Jawab : Ya, top-k kecil sudah cukup.

---

### 3ï¸âƒ£ Konten final ke LLM

```python
FINAL_TOP_M = 5
```

**Fungsi**
Jumlah konteks yang akhirnya dimasukkan ke prompt LLM.

| Arah tuning        | Efek                                                                                                                                                                                        |
| ------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Naikkan** (6â€“8)  | LLM menerima lebih banyak potongan informasi. Jawaban jadi lebih lengkap dan aman dari â€œkonteks hilangâ€, tapi kadang membuat respon lebih panjang atau memasukkan info yang kurang relevan. |
| **Turunkan** (2â€“3) | LLM hanya memakai sedikit konteks. Jawaban jadi lebih ringkas dan langsung ke poin, tapi ada risiko bagian penting tidak ikut terbawa.                                                      |

**Pertanyaan**

- Benarkah â€œsemakin banyak konteks semakin bagusâ€?
- Jawab : Lebih banyak konteks bisa membantu, tapi tidak berarti selalu lebih baik. Kalau konteks terlalu banyak, jawabannya bisa jadi melebar.

---

### 4ï¸âƒ£ Cross-Encoder Re-ranker

```python
USE_RERANKER = True
```

**Fungsi**
Menilai ulang (rerank) dokumen hasil embedding.

| Mode    | Efek                                                                                                                                                                                           |
| ------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `True`  | Sistem mengecek ulang, lalu memilih mana yang paling tepat sebelum menjawab. Hasilnya biasanya lebih akurat untuk pertanyaan yang rumit atau membingungkan, tapi waktu proses jadi lebih lama. |
| `False` | Sistem langsung memakai hasil tanpa dicek ulang. Prosesnya lebih cepat, tapi bisa saja mengambil dokumen yang kurang tepat untuk pertanyaan tertentu.                                          |

**Pertanyaan**

- Pada query jenis apa reranker paling terasa manfaatnya?
- Jawab : Reranker paling terasa manfaatnya pada pertanyaan yang lebih sulit, Contohnya seperti pertanyaan kamu: â€œMengapa Pertamina RU III mengurusi ikan Belida?â€. Ini bukan pertanyaan sederhana. Jawabannya tersebar di beberapa bagian dokumen, dan membutuhkan pemahaman konteks (CSR, edukasi, pelestarian, studi kasus, dan alasan pemilihan lokasi).

---

### 5ï¸âƒ£ Temperatur generasi

```python
TEMPERATURE = 0.2
```

**Fungsi**
Mengontrol variasi / kreativitas jawaban.

| Value   | Efek                                                                                                                                             |
| ------- | ------------------------------------------------------------------------------------------------------------------------------------------------ |
| 0.0â€“0.2 | Jawaban tidak bertele-tele, langsung ke poin, dan mengikuti dokumen. Cocok untuk jawaban faktual atau formal.                                    |
| 0.5â€“0.7 | Jawaban lebih bervariasi, bahasanya lebih bebas tetapi bisa sedikit melebar dari dokumen. Cocok untuk penjelasan yang lebih santai atau kreatif. |

**Pertanyaan**

- Untuk QA berbasis dokumen perusahaan, temperatur ideal berapa?
- Jawab : 0.0â€“0.2 alasannya karena QA berbasis dokumen perusahaan harus konsisten, tidak boleh menambah informasi yang tidak ada di dokumen, dan harus menjaga â€œgaya formalâ€ perusahaan.

---

### 6ï¸âƒ£ Model LLM Groq

```python
GROQ_MODEL = "llama-3.1-8b-instant"
# alternatif:
# gpt-oss-120b
# llama-3.3-70b-versatile
```

| Pilihan                  | Efek                                                                                                      |
| ------------------------ | --------------------------------------------------------------------------------------------------------- |
| Model kecil (8B)         | Lebih cepat, ringan, dan responsnya stabil namun pemahaman konteksnya tidak sedalam model besar.          |
| Model besar (70B / 120B) | Jawaban lebih detail dan lebih â€œpintarâ€ dalam memahami konteks yang rumit. Namun waktu proses lebih lama. |

**Pertanyaan**

- Apakah peningkatan kualitas sebanding dengan penambahan waktu?
- Jawab : Tidak selalu. Model besar hanya terasa lebih berguna pada pertanyaan yang benar-benar rumit atau membutuhkan penalaran yang tinggi.

---

## Tips

- Pakai PDF minimal 1 halaman, maksimal 10 halaman, biar gak berat.
- Coba pertanyaan definisi, konteks, dan reasoning
- Catat kapan sistem salah & mengapa
- Prioritaskan kualitas hasil dalam laporan

---
